import os
import sys
import argparse
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP

from engine_finetune_raddet_unified import train_one_epoch, validate_one_epoch
from models.RCRODNet import RCRODNet
from dataset_raddet_adapter import RADDetDataset


# ==========================================================
# åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
# ==========================================================
def setup_distributed(local_rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼é€šä¿¡ç¯å¢ƒ (ä»… Linux ä½¿ç”¨)"""
    if sys.platform.startswith("win"):
        print("âš ï¸ Windows ç¯å¢ƒä¸æ”¯æŒ NCCLï¼Œè·³è¿‡ DDP åˆå§‹åŒ–ã€‚")
        return False

    torch.cuda.set_device(local_rank)
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=local_rank
    )
    return True


# ==========================================================
# é”€æ¯åˆ†å¸ƒå¼ç¯å¢ƒ
# ==========================================================
def cleanup_distributed():
    if dist.is_initialized():
        dist.destroy_process_group()


# ==========================================================
# ä¸»è®­ç»ƒå‡½æ•°ï¼ˆå•å¡æˆ–å¤šå¡ï¼‰
# ==========================================================
def main_worker(local_rank, args):
    # è‡ªåŠ¨è¯†åˆ«æ˜¯å¦ä½¿ç”¨ DDP
    use_ddp = torch.cuda.is_available() and torch.cuda.device_count() > 1 and not sys.platform.startswith("win")
    device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")

    if use_ddp:
        dist.init_process_group(backend='nccl', init_method='env://')
        print(f"âœ… [DDP] Using GPU {local_rank}/{torch.cuda.device_count()}")
        torch.cuda.set_device(local_rank)
    else:
        print("âš ï¸ [Single GPU / CPU] Running without DDP.")
        local_rank = 0

    # ===================== æ•°æ®é›†åŠ è½½ =====================
    train_dataset = RADDetDataset(
        root=args.train_data,
        split='train'
    )
    val_dataset = RADDetDataset(
        root=args.val_data,
        split='val'
    )

    if use_ddp:
        train_sampler = DistributedSampler(train_dataset)
        val_sampler = DistributedSampler(val_dataset, shuffle=False)
    else:
        train_sampler = None
        val_sampler = None

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        shuffle=(train_sampler is None),
        num_workers=args.num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        sampler=val_sampler,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )

    # ===================== æ¨¡å‹å®šä¹‰ =====================
    model = RCRODNet(in_channels=4, num_classes=args.num_classes)
    model = model.to(device)

    if use_ddp:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    # ===================== ä¼˜åŒ–å™¨å®šä¹‰ =====================
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    # ===================== è®­ç»ƒ/éªŒè¯å¾ªç¯ =====================
    for epoch in range(args.epochs):
        if use_ddp:
            train_sampler.set_epoch(epoch)

        train_one_epoch(model, train_loader, optimizer, device, epoch, args)
        validate_one_epoch(model, val_loader, device, epoch, args)

        scheduler.step()

        if local_rank == 0:
            ckpt_path = os.path.join(args.save_dir, f"raddet_epoch_{epoch+1}.pth")
            torch.save(model.state_dict(), ckpt_path)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {ckpt_path}")

    cleanup_distributed()


# ==========================================================
# ä¸»å…¥å£
# ==========================================================
def main():
    parser = argparse.ArgumentParser(description="Finetune RADDet (Single/Distributed)")
    parser.add_argument('--train_data', type=str, default="datasets/RADDet/train")
    parser.add_argument('--val_data', type=str, default="datasets/RADDet/val")
    parser.add_argument('--save_dir', type=str, default="checkpoints/raddet_finetune")
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--num_classes', type=int, default=10)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--world_size', type=int, default=torch.cuda.device_count())
    args = parser.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)

    if torch.cuda.device_count() > 1 and not sys.platform.startswith("win"):
        print(f"ğŸŒ å¯åŠ¨ DDP æ¨¡å¼ï¼Œå…± {args.world_size} å¼  GPUã€‚")
        mp.spawn(main_worker, nprocs=args.world_size, args=(args,))
    else:
        print("ğŸ–¥ï¸ å¯åŠ¨å• GPU / CPU æ¨¡å¼ã€‚")
        main_worker(local_rank=0, args=args)


if __name__ == "__main__":
    main()
