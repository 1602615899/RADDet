import torch
import torch.nn as nn
from functools import partial
from torch import Tensor
from typing import Optional, Tuple
import math
import numpy as np
import torch.nn.functional as F
from timm.models.vision_transformer import _load_weights
from fvcore.nn import FlopCountAnalysis, parameter_count_table
from timm.models.vision_transformer import _cfg
from timm.models.registry import register_model
from timm.models.layers import trunc_normal_, lecun_normal_, DropPath
from einops import rearrange, repeat

from mamba_ssm.modules.mamba_simple import Mamba 
try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None

# ================= FiLM æ¨¡å—å®šä¹‰ =================
class FiLMLayer(nn.Module):
    """
    ç‰¹å¾çº§çº¿æ€§è°ƒåˆ¶å±‚ (FiLM Layer)ã€‚
    è¯¥å±‚æ ¹æ®ä¸€ä¸ªå¤–éƒ¨çš„æ¡ä»¶å‘é‡ï¼Œç”Ÿæˆç¼©æ”¾å› å­gammaå’Œåç§»å› å­betaï¼Œ
    å¹¶å°†å…¶åº”ç”¨äºè¾“å…¥çš„ç‰¹å¾å›¾ã€‚
    """
    def __init__(self, condition_dim, feature_dim):
        """
        åˆå§‹åŒ–FiLMå±‚ã€‚
        Args:
            condition_dim (int): æ¡ä»¶å‘é‡çš„ç»´åº¦ã€‚
            feature_dim (int): è¦è°ƒåˆ¶çš„ç‰¹å¾çš„ç»´åº¦ã€‚
        """
        super().__init__()
        # ä¸€ä¸ªç®€å•çš„å‰é¦ˆç½‘ç»œï¼Œç”¨äºä»æ¡ä»¶å‘é‡ç”Ÿæˆgammaå’Œbeta
        self.generator = nn.Sequential(
            nn.Linear(condition_dim, condition_dim * 2),
            nn.GELU(),
            nn.Linear(condition_dim * 2, feature_dim * 2) # è¾“å‡ºç»´åº¦æ˜¯ç‰¹å¾ç»´åº¦çš„ä¸¤å€ (gamma + beta)
        )

    def forward(self, x, cond_vector):
        """
        å‰å‘ä¼ æ’­ã€‚
        Args:
            x (Tensor): è¾“å…¥ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [B, N, C] (Batch, Num_Tokens, Channels)ã€‚
            cond_vector (Tensor): æ¡ä»¶å‘é‡ï¼Œå½¢çŠ¶ä¸º [B, D] (Batch, Condition_Dim)ã€‚
        
        Returns:
            Tensor: ç»è¿‡è°ƒåˆ¶çš„ç‰¹å¾ï¼Œå½¢çŠ¶ä¸xç›¸åŒã€‚
        """
        # 1. ç”Ÿæˆ gamma å’Œ beta
        # generatorçš„è¾“å‡ºå½¢çŠ¶ä¸º [B, C*2]
        gb = self.generator(cond_vector)
        
        # å°†gbåˆ‡åˆ†ä¸ºgammaå’Œbeta
        gamma, beta = torch.chunk(gb, 2, dim=1) # æ¯ä¸ªå½¢çŠ¶ä¸º [B, C]
        
        # 2. è°ƒæ•´å½¢çŠ¶ä»¥è¿›è¡Œå¹¿æ’­
        # [B, C] -> [B, 1, C]ï¼Œä»¥ä¾¿ä¸ [B, N, C] çš„xè¿›è¡Œå…ƒç´ çº§æ“ä½œ
        gamma = gamma.unsqueeze(1)
        beta = beta.unsqueeze(1)

        # 3. åº”ç”¨FiLMå˜æ¢: x' = Î³ * x + Î²
        # ä¸ºäº†è®­ç»ƒç¨³å®šæ€§ï¼Œé€šå¸¸åˆå§‹åŒ–æ—¶è®©gammaæ¥è¿‘1ï¼Œbetaæ¥è¿‘0ã€‚
        # è¿™é‡Œé€šè¿‡åŠ 1æ¥å®ç°ï¼Œä½¿å¾—åœ¨ç½‘ç»œåˆå§‹åŒ–æ—¶ï¼ŒFiLMå±‚è¿‘ä¼¼äºä¸€ä¸ªæ’ç­‰å˜æ¢ã€‚
        return (gamma + 1) * x + beta

# ================= å…ƒæ•°æ®ç¼–ç å™¨ =================
class MetadataEncoder(nn.Module):
    """
    å°†é›·è¾¾å…ƒæ•°æ®ç¼–ç ä¸ºFiLMæ‰€éœ€çš„æ¡ä»¶å‘é‡ã€‚
    """
    def __init__(self, num_datasets, numerical_dim, output_dim):
        """
        åˆå§‹åŒ–å…ƒæ•°æ®ç¼–ç å™¨ã€‚
        Args:
            num_datasets (int): æ•°æ®é›†ç±»å‹çš„æ•°é‡ (ä¾‹å¦‚: RADDet, CARRADAç­‰)ã€‚
            numerical_dim (int): æ•°å€¼å‹å…ƒæ•°æ®çš„ç»´åº¦ (ä¾‹å¦‚: [range_res, angle_res, vel_res])ã€‚
            output_dim (int): è¾“å‡ºæ¡ä»¶å‘é‡çš„ç›®æ ‡ç»´åº¦ã€‚
        """
        super().__init__()
        # 1. ç±»åˆ«å‹æ•°æ®ç¼–ç å™¨
        self.dataset_embedding_dim = 32 # ä¸ºæ•°æ®é›†IDåˆ†é…çš„åµŒå…¥ç»´åº¦
        self.dataset_embedder = nn.Embedding(num_datasets, self.dataset_embedding_dim)

        # 2. æ•°å€¼å‹æ•°æ®ç¼–ç å™¨ (ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚)
        self.numerical_proj_dim = 64
        self.numerical_projector = nn.Linear(numerical_dim, self.numerical_proj_dim)

        # 3. èåˆå±‚
        # å°†ç¼–ç åçš„ç±»åˆ«å‹å’Œæ•°å€¼å‹ç‰¹å¾æ‹¼æ¥åï¼Œé€šè¿‡ä¸€ä¸ªMLPè¿›è¡Œèåˆ
        total_input_dim = self.dataset_embedding_dim + self.numerical_proj_dim
        self.fusion_mlp = nn.Sequential(
            nn.Linear(total_input_dim, output_dim * 2),
            nn.GELU(),
            nn.Linear(output_dim * 2, output_dim)
        )

    def forward(self, dataset_ids, numerical_params):
        """
        å‰å‘ä¼ æ’­ã€‚
        Args:
            dataset_ids (Tensor): æ•°æ®é›†IDï¼Œå½¢çŠ¶ [B]ã€‚
            numerical_params (Tensor): æ•°å€¼å‹å‚æ•°ï¼Œå½¢çŠ¶ [B, numerical_dim]ã€‚

        Returns:
            Tensor: è¾“å‡ºçš„æ¡ä»¶å‘é‡ï¼Œå½¢çŠ¶ [B, output_dim]ã€‚
        """
        # ç¼–ç ç±»åˆ«å‹æ•°æ®
        dataset_emb = self.dataset_embedder(dataset_ids) # [B, 32]

        # ç¼–ç æ•°å€¼å‹æ•°æ®
        numerical_proj = self.numerical_projector(numerical_params) # [B, 64]
        
        # æ‹¼æ¥
        combined_features = torch.cat([dataset_emb, numerical_proj], dim=1) # [B, 32+64]

        # èåˆç”Ÿæˆæœ€ç»ˆçš„æ¡ä»¶å‘é‡
        cond_vector = self.fusion_mlp(combined_features) # [B, output_dim]
        
        return cond_vector

def get_3d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: ç½‘æ ¼é«˜åº¦ã€å®½åº¦å’Œæ·±åº¦çš„æ•´æ•°å…ƒç»„
    è¿”å›:
    pos_embed: [grid_depth*grid_height*grid_width, embed_dim] æˆ– [1+grid_depth*grid_height*grid_width, embed_dim] (å¸¦cls_token)
    """
    grid_d = np.arange(grid_size[0], dtype=np.float32)
    grid_h = np.arange(grid_size[1], dtype=np.float32)
    grid_w = np.arange(grid_size[2], dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h, grid_d)  # æ³¨æ„è¿™é‡Œçš„w, h, dé¡ºåºå¾ˆé‡è¦
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([3, 1, grid_size[1], grid_size[2], grid_size[0]])
    pos_embed = get_3d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed

def get_3d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 6 == 0

    # ä½¿ç”¨1/3ç»´åº¦ç»™grid_hï¼Œ1/3ç»™grid_wï¼Œ1/3ç»™grid_d
    emb_d = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[0])  # (H*W*D, D/3)
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[1])  # (H*W*D, D/3)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[2])  # (H*W*D, D/3)

    emb = np.concatenate([emb_d, emb_h, emb_w], axis=1) # (H*W*D, D)
    return emb

def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: æ¯ä¸ªä½ç½®çš„è¾“å‡ºç»´åº¦
    pos: è¦ç¼–ç çš„ä½ç½®åˆ—è¡¨ï¼šå¤§å° (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), å¤–ç§¯

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb

def get_3d_physical_pos_embed_dynamic(embed_dim, physical_coords):
    """
    åŠ¨æ€ç”Ÿæˆç‰©ç†ä½ç½®ç¼–ç ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰
    physical_coords: [batch_size, num_tokens, 3] ç‰©ç†åæ ‡ (è·ç¦», è§’åº¦, é€Ÿåº¦/æ—¶é—´)
    è¿”å›: [batch_size, num_tokens, embed_dim]
    """
    batch_size, num_tokens, _ = physical_coords.shape
    device = physical_coords.device
    
    # ç¡®ä¿ç»´åº¦èƒ½å¤Ÿå‡åŒ€åˆ†é…ç»™ä¸‰ä¸ªåæ ‡ç»´åº¦
    dim_per_coord = embed_dim // 3
    remaining_dims = embed_dim % 3
    
    pos_embed = torch.zeros(batch_size, num_tokens, embed_dim, device=device)
    
    start_dim = 0
    for j in range(3):  # è·ç¦»ã€è§’åº¦ã€é€Ÿåº¦/æ—¶é—´
        # ä¸ºå½“å‰åæ ‡åˆ†é…ç»´åº¦ï¼ˆç¬¬ä¸€ä¸ªåæ ‡è·å¾—é¢å¤–ç»´åº¦ï¼‰
        current_dim = dim_per_coord + (1 if j < remaining_dims else 0)
        
        if current_dim == 0:
            continue
            
        end_dim = start_dim + current_dim
        
        # ç”Ÿæˆé¢‘ç‡å‚æ•°ï¼ˆç¡®ä¿æ˜¯å¶æ•°ç»´åº¦ç”¨äºsin/coså¯¹ï¼‰
        if current_dim % 2 == 1:
            # å¦‚æœæ˜¯å¥‡æ•°ç»´åº¦ï¼Œä½¿ç”¨current_dim-1æ¥ç”Ÿæˆé¢‘ç‡
            freq_dim = current_dim - 1
            use_extra_dim = True
        else:
            freq_dim = current_dim
            use_extra_dim = False
            
        if freq_dim > 0:
            omega = torch.arange(freq_dim // 2, dtype=torch.float32, device=device)
            omega /= freq_dim / 2.
            omega = 1. / 10000**omega  # (freq_dim/2,)
            
            coord_data = physical_coords[:, :, j]  # [batch_size, num_tokens]
            
            # æ‰¹é‡è®¡ç®— (å®Œå…¨å‘é‡åŒ–)
            out = torch.einsum('bn,d->bnd', coord_data, omega)  # (batch_size, num_tokens, freq_dim/2)
            emb_sin = torch.sin(out)
            emb_cos = torch.cos(out)
            combined_emb = torch.cat([emb_sin, emb_cos], dim=-1)  # (batch_size, num_tokens, freq_dim)
        else:
            combined_emb = torch.zeros(batch_size, num_tokens, 0, device=device)
        
        # å¦‚æœéœ€è¦é¢å¤–ç»´åº¦ï¼Œæ·»åŠ ä¸€ä¸ªç®€å•çš„ç¼–ç 
        if use_extra_dim:
            extra_emb = physical_coords[:, :, j:j+1] * 0.01  # ç®€å•çš„çº¿æ€§ç¼–ç 
            combined_emb = torch.cat([combined_emb, extra_emb], dim=-1)
        
        pos_embed[:, :, start_dim:end_dim] = combined_emb
        start_dim = end_dim
    
    return pos_embed  # [batch_size, num_tokens, embed_dim]

def generate_physical_coords_fully_vectorized(batch_params_tensor, grid_size, has_velocity_mask, device):
    """
    å®Œå…¨å‘é‡åŒ–çš„ç‰©ç†åæ ‡ç”Ÿæˆï¼ˆæ— ä»»ä½•Pythonå¾ªç¯ï¼‰
    batch_params_tensor: [B, 4] çš„å¼ é‡, [range_res, angle_res, vel_res/time_res, is_velocity_flag]
    grid_size: (H, W, D)
    has_velocity_mask: [B] å¸ƒå°”å¼ é‡ï¼Œæ ‡è¯†æ¯ä¸ªæ ·æœ¬æ˜¯å¦æœ‰é€Ÿåº¦ç»´åº¦
    device: è®¡ç®—è®¾å¤‡
    è¿”å›: [batch_size, num_tokens, 3]
    """
    B = batch_params_tensor.shape[0]
    H, W, D = grid_size
    
    # 1. åˆ›å»ºç´¢å¼•ç½‘æ ¼ (h, w, d) - æ‰©å±•åˆ° [B, H, W, D]
    h_indices = torch.arange(H, device=device).view(1, H, 1, 1).expand(B, H, W, D)  # [B, H, W, D]
    w_indices = torch.arange(W, device=device).view(1, 1, W, 1).expand(B, H, W, D)  # [B, H, W, D]
    d_indices = torch.arange(D, device=device).view(1, 1, 1, D).expand(B, H, W, D)  # [B, H, W, D]
    
    # 2. å‡†å¤‡ç‰©ç†åˆ†è¾¨ç‡å¼ é‡ä»¥è¿›è¡Œå¹¿æ’­
    range_res = batch_params_tensor[:, 0].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    angle_res = batch_params_tensor[:, 1].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    vel_or_time_res = batch_params_tensor[:, 2].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    
    # 3. ä½¿ç”¨å¹¿æ’­æœºåˆ¶ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åæ ‡ - å®Œå…¨å‘é‡åŒ–
    distances = h_indices * range_res  # [B, H, W, D]
    angles = (w_indices - W / 2) * angle_res  # [B, H, W, D]
    
    # å¤„ç†é€Ÿåº¦/æ—¶é—´ç»´åº¦çš„æ¡ä»¶å¹¿æ’­
    velocities = torch.where(
        has_velocity_mask.view(B, 1, 1, 1),
        (d_indices - D / 2) * vel_or_time_res,  # é€Ÿåº¦æ¨¡å¼
        d_indices * vel_or_time_res  # æ—¶é—´æ¨¡å¼ (CRUW)
    )  # [B, H, W, D]
    
    # 4. å †å å¹¶å±•å¹³ - å®Œå…¨å‘é‡åŒ–
    coords = torch.stack([distances, angles, velocities], dim=-1).view(B, -1, 3)
    
    # 5. æ‰¹å¤„ç†å½’ä¸€åŒ– (å…³é”®) - å®Œå…¨å‘é‡åŒ–
    max_vals = coords.max(dim=1, keepdim=True)[0]  # [B, 1, 3]
    min_vals = coords.min(dim=1, keepdim=True)[0]  # [B, 1, 3]
    range_vals = max_vals - min_vals
    range_vals = torch.where(range_vals > 0, range_vals, torch.ones_like(range_vals))
    
    coords_normalized = (coords - min_vals) / range_vals
    
    return coords_normalized

def prepare_batch_params_tensor(batch_params, device):
    """
    [ä¿®æ”¹åçš„ç‰ˆæœ¬]
    å°†å¼ é‡å­—å…¸æ ¼å¼çš„å‚æ•°ç›´æ¥è½¬æ¢ä¸ºæ‰€éœ€å¼ é‡ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰ã€‚
    batch_params: æ¯ä¸ªé”®éƒ½å¯¹åº”ä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º [B]ã€‚
    è¿”å›: [B, 4] å¼ é‡å’Œ [B] å¸ƒå°”æ©ç ã€‚
    """
    # ä»å­—å…¸ä¸­ç›´æ¥è·å–æ•´æ‰¹çš„å¼ é‡
    range_res = batch_params['range_resolution'].to(device)
    angle_res = batch_params['angular_resolution'].to(device)
    
    B = range_res.shape[0]
    default_vel_res = torch.full((B,), 0.05, device=device)
    vel_or_time_res = batch_params.get('velocity_resolution', batch_params.get('time_res', default_vel_res)).to(device)
    
    default_has_vel = torch.zeros(B, dtype=torch.bool, device=device)
    has_velocity = batch_params.get('has_velocity', default_has_vel).to(device)

    # åˆ›å»ºé€Ÿåº¦æ©ç å’Œé€Ÿåº¦æ ‡è®°
    has_velocity_mask = has_velocity.bool() 
    is_velocity_flag = has_velocity_mask.float()

    # ä½¿ç”¨ torch.stack å°†æ‰€æœ‰å‚æ•°å¼ é‡å †å èµ·æ¥
    batch_params_tensor = torch.stack([
        range_res.float(),
        angle_res.float(),
        vel_or_time_res.float(),
        is_velocity_flag
    ], dim=1)

    return batch_params_tensor, has_velocity_mask


class singleLayerHead3D(nn.Module):
    def __init__(self, num_anchors, num_class, channel, **kwargs):
        super(singleLayerHead3D, self).__init__()
        self.num_anchor = num_anchors
        self.num_class = num_class
        self.channel = channel

        final_output_channels = int(self.num_anchor * (num_class + 7))  # 78

        self.conv1 = nn.Conv3d(in_channels=self.channel,
                               out_channels=self.channel*2,
                               kernel_size=3,
                               stride=1,
                               padding=1,
                               bias=True)
        self.bn1 = nn.BatchNorm3d(self.channel*2)

        self.conv2 = nn.Conv3d(in_channels=self.channel*2,
                               out_channels=final_output_channels,
                               kernel_size=1,
                               stride=1,
                               bias=True)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, input_feature):
        x = self.relu(self.bn1(self.conv1(input_feature)))
        x = self.conv2(x)
        return x

# ========= FPNç›¸å…³è¾…åŠ©ç±» (æ¥è‡ªradtr_yolo.py) =========
class ConvBNAct3D(nn.Sequential):
    """Standard Conv3D -> BatchNorm3d -> Activation block."""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, activation=nn.SiLU):
        padding = (kernel_size - 1) // 2
        super().__init__(
            nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False),
            nn.BatchNorm3d(out_channels),
            activation(inplace=True)
        )

class ConvDownsample3D(ConvBNAct3D):
    """ConvBNAct3D with stride=2 for downsampling."""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super().__init__(in_channels, out_channels, kernel_size, stride=2)

class PANetFusion3D(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)
        self.downsample = ConvDownsample3D(embed_dim, embed_dim, kernel_size=3)

        self.lat_f0 = nn.Conv3d(embed_dim, embed_dim//4, kernel_size=1)
        self.lat_d1 = nn.Conv3d(embed_dim, embed_dim//2, kernel_size=1)
        self.lat_d2 = nn.Conv3d(embed_dim, embed_dim//2, kernel_size=1)

        self.fuse_td1 = ConvBNAct3D(embed_dim//2, embed_dim//4, kernel_size=3)
        self.fuse_td0 = ConvBNAct3D(embed_dim//4, embed_dim//4, kernel_size=3)

        self.downsample_bu1 = ConvDownsample3D(embed_dim//4, embed_dim//4, kernel_size=3)
        self.downsample_bu2 = ConvDownsample3D(embed_dim//2, embed_dim//2, kernel_size=3)

        self.fuse_bu1 = ConvBNAct3D(embed_dim//4, embed_dim//2, kernel_size=3)
        self.fuse_bu2 = ConvBNAct3D(embed_dim//2, embed_dim, kernel_size=3)

    def forward(self, features):
        # Input: list [f0, f1, f2], all initially [B, C, D, H, W]
        f0, f1, f2 = features

        # --- Create Multi-Scale Features ---
        # Upsample f0 to create d0
        d0 = self.upsample(f0)
        # Downsample f2 to create d2
        d2 = self.downsample(f2)

        # --- Top-Down Path ---
        # Apply lateral connections
        lat0 = self.lat_f0(d0)
        lat1 = self.lat_d1(f1)
        lat2 = self.lat_d2(d2)

        # P2 (from smallest feature map d2)
        p2 = lat2 # Feature map at the smallest scale
        # P1 = Upsample(P2) + Lateral(f1) -> Refine
        p1_sum = self.upsample(p2) + lat1
        p1 = self.fuse_td1(p1_sum)
        # P0 = Upsample(P1) + Lateral(d0) -> Refine
        p0_sum = self.upsample(p1) + lat0
        p0 = self.fuse_td0(p0_sum)
        # --- Bottom-Up Path ---
        # N0 is the same as P0 for the highest resolution
        n0 = p0

        # N1 = Downsample(N0) + P1 -> Refine
        n1_sum = self.downsample_bu1(n0) + p1 # Reuse p1 from top-down
        n1 = self.fuse_bu1(n1_sum)
        
        # N2 = Downsample(N1) + P2 -> Refine
        n2_sum = self.downsample_bu2(n1) + p2 # Reuse p2 from top-down
        n2 = self.fuse_bu2(n2_sum)

        return n0, n1, n2 # Shape should be [B, C, D, H, W] (same as f0)
    
class PatchEmbed3D(nn.Module):
    """RAD tensor to 3D(RAD) Patch Embedding"""
    def __init__(self, input_size=(256, 256, 64), patch_size=(16, 16, 16), in_chans=1, embed_dim=192):
        super().__init__()
        self.input_size = input_size
        self.patch_size = patch_size
        
        # åŠ¨æ€è®¡ç®—ç½‘æ ¼å¤§å°
        self.grid_size = (
            input_size[0] // patch_size[0],  # H
            input_size[1] // patch_size[1],  # W
            input_size[2] // patch_size[2]   # D
        )
        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]

        self.proj = nn.Conv3d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        # è¾“å…¥xå½¢çŠ¶: [B, C, H, W, D]
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class SwiGLU(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.,
                 norm_layer=nn.LayerNorm, subln=False):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.w1 = nn.Linear(in_features, hidden_features)
        self.w2 = nn.Linear(in_features, hidden_features)

        self.act = act_layer()
        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()
        self.w3 = nn.Linear(hidden_features, out_features)

        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x1 = self.w1(x)
        x2 = self.w2(x)
        hidden = self.act(x1) * x2
        x = self.ffn_ln(hidden)
        x = self.w3(x)
        x = self.drop(x)
        return x

class ARBlock(nn.Module):
    def __init__(
        self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.,
        fused_add_norm=False, residual_in_fp32=False,
        use_film=False, condition_dim=None
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.mlp = SwiGLU(dim, dim*4*2//3, subln=False)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

        self.use_film = use_film
        if self.use_film:
            assert condition_dim is not None, "condition_dim must be provided if use_film is True"
            self.film_layer = FiLMLayer(condition_dim=condition_dim, feature_dim=dim)


    def forward(self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, cond_vector: Optional[Tensor] = None):
        normed_states = self.norm1(hidden_states)
        
        if self.use_film and cond_vector is not None:
            modulated_states = self.film_layer(normed_states, cond_vector)
        else:
            modulated_states = normed_states

        mixer_out = self.mixer(
            modulated_states, 
            inference_params=inference_params
        )

        hidden_states = hidden_states + self.drop_path(mixer_out)
        hidden_states = hidden_states + self.drop_path(self.mlp(self.norm2(hidden_states)))

        return hidden_states

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)

def create_block(
    d_model,
    ssm_cfg=None,
    norm_epsilon=1e-5,
    drop_path=0.,
    rms_norm=False,
    residual_in_fp32=False,
    fused_add_norm=False,
    layer_idx=None,
    device=None,
    dtype=None,
    if_bimamba=False,
    bimamba_type="none",
    if_devide_out=False,
    init_layer_scale=None,
    use_film=False,
    condition_dim=None,
):
    if if_bimamba:
        bimamba_type = "v1"
    if ssm_cfg is None:
        ssm_cfg = {}
    factory_kwargs = {"device": device, "dtype": dtype}
    mixer_cls = partial(Mamba, expand=1, layer_idx=layer_idx, bimamba_type=bimamba_type, if_divide_out=if_devide_out, init_layer_scale=init_layer_scale, **ssm_cfg, **factory_kwargs)
    norm_cls = partial(
        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
    )
    block = ARBlock(
        d_model,
        mixer_cls,
        norm_cls=norm_cls,
        drop_path=drop_path,
        fused_add_norm=fused_add_norm,
        residual_in_fp32=residual_in_fp32,
        use_film=use_film,
        condition_dim=condition_dim,
    )
    block.layer_idx = layer_idx
    return block

# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
def _init_weights(
    module,
    n_layer,
    initializer_range=0.02,  # Now only used for embedding layer.
    rescale_prenorm_residual=True,
    n_residuals_per_layer=1,  # Change to 2 if we have MLP
):
    if isinstance(module, nn.Linear):
        if module.bias is not None:
            if not getattr(module.bias, "_no_reinit", False):
                nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.
        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
        #
        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
                # We need to reinit p since this code could be called multiple times
                # Having just p *= scale would repeatedly scale it down
                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
                with torch.no_grad():
                    p /= math.sqrt(n_residuals_per_layer * n_layer)


def segm_init_weights(m):
    if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        lecun_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(m.bias)
        nn.init.ones_(m.weight)


# ========= ä¸»è¦å¾®è°ƒæ¨¡å‹ =========
class RadDet(nn.Module):

    def __init__(self, 
                 config_data,
                 config_model,
                 anchor_boxes,
                 embed_dim=192,
                 depth=12,
                 ssm_cfg=None, 
                 drop_path_rate=0,
                 modality_embed_dim=64, # ### FIX 2: æ·»åŠ  modality_embed_dim å‚æ•°
                 norm_epsilon: float = 1e-5, 
                 rms_norm: bool = False, 
                 initializer_cfg=None,
                 fused_add_norm=False,
                 residual_in_fp32=False,
                 device=None,
                 dtype=None,
                 if_bimamba=False,
                 bimamba_type="none",
                 if_devide_out=False,
                 init_layer_scale=None,
                 use_film_metadata=True,
                 condition_dim=12,
                 **kwargs):
        
        factory_kwargs = {"device": device, "dtype": dtype}
        kwargs.update(factory_kwargs) 
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        # ä¿å­˜æ¨¡å‹é…ç½®
        self.config_data = config_data
        self.config_model = config_model
        self.anchor_boxes = anchor_boxes
        self.embed_dim = embed_dim  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
        self.depth = depth          # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
        
        # æ¨¡å‹å‚æ•° - æ”¯æŒé…ç½®
        input_size = (256, 256, 64)
        patch_size = (8, 8, 8)  # ç»Ÿä¸€ä¸º(16, 16, 16)ï¼Œä¸é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸€è‡´
        # embed_dim å’Œ depth ç°åœ¨ä½œä¸ºå‚æ•°ä¼ å…¥
        
        print(f"å¾®è°ƒæ¨¡å‹åˆå§‹åŒ– - embed_dim: {embed_dim}, depth: {depth}")
        
        # Patch Embedding
        self.patch_embed = PatchEmbed3D(
            input_size=input_size,
            patch_size=patch_size, 
            in_chans=1,  # RADDetä½¿ç”¨1ä¸ªé€šé“
            embed_dim=embed_dim
        )
        # self.pos_drop = nn.Dropout(p=pos_drop_rate) # ### FIX 1: åˆå§‹åŒ– pos_drop
        # åŠ¨æ€è®¡ç®—ç½‘æ ¼å¤§å°å’Œåˆ†ç»„å¤§å°
        self.grid_size = (
            input_size[0] // patch_size[0],
            input_size[1] // patch_size[1],
            input_size[2] // patch_size[2]
        )
        self.group_size = (
            min(4, self.grid_size[0]),
            min(4, self.grid_size[1]),
            min(4, self.grid_size[2])
        )
        print(f"ç½‘æ ¼å¤§å°: {self.grid_size}, åˆ†ç»„å¤§å°: {self.group_size}")
        self.register_buffer("grid_tensor", torch.tensor(self.grid_size, dtype=torch.long))
        self.register_buffer("group_tensor", torch.tensor(self.group_size, dtype=torch.long))
        
        # æ¨¡æ€ç±»å‹åµŒå…¥ (ä¿æŒåˆ†è¾¨ç‡ä¿¡æ¯)
        self.modality_embedding = nn.Embedding(2, modality_embed_dim)
        
        # è°ƒæ•´ç‰©ç†ä½ç½®ç¼–ç ç»´åº¦
        self.physical_pos_embed_dim = embed_dim - modality_embed_dim
        assert self.physical_pos_embed_dim > 0, "æ¨¡æ€åµŒå…¥ç»´åº¦ä¸èƒ½å¤§äºç­‰äºæ€»åµŒå…¥ç»´åº¦"
        print(f"ç‰©ç†ä½ç½®ç¼–ç ç»´åº¦: {self.physical_pos_embed_dim}, æ¨¡æ€åµŒå…¥ç»´åº¦: {modality_embed_dim}")
            

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        # import ipdb;ipdb.set_trace()
        inter_dpr = [0.0] + dpr    
        self.layers = nn.ModuleList(
            [
                create_block(
                    embed_dim,
                    ssm_cfg=ssm_cfg,
                    norm_epsilon=norm_epsilon,
                    rms_norm=rms_norm,
                    residual_in_fp32=residual_in_fp32,
                    fused_add_norm=fused_add_norm,
                    layer_idx=i,
                    if_bimamba=if_bimamba,
                    bimamba_type=bimamba_type,
                    drop_path=inter_dpr[i],
                    if_devide_out=if_devide_out,
                    init_layer_scale=init_layer_scale,
                    use_film=use_film_metadata,
                    condition_dim=condition_dim, 
                    **factory_kwargs,
                )
                for i in range(depth)
            ]
        )
        self.depth = depth
        if self.depth == 12:
            self.feature_indices = [3, 7, 11] # å¯¹åº”ç¬¬4, 8, 12å±‚
        elif self.depth == 24:
            self.feature_indices = [7, 15, 23]
        # self.norm_layers = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(len(self.feature_indices))])

        self.feature_fusion = PANetFusion3D(embed_dim=embed_dim)
        
        # YOLOæ£€æµ‹å¤´ - ä½¿ç”¨å®Œæ•´çš„embed_dimè¾“å…¥é€šé“
        self.yolo_head = singleLayerHead3D(num_anchors=6, num_class=6, channel=embed_dim//4)
        self.yolo_head1 = singleLayerHead3D(num_anchors=6, num_class=6, channel=embed_dim//2)
        self.yolo_head2 = singleLayerHead3D(num_anchors=6, num_class=6, channel=embed_dim)

        # åˆå§‹åŒ–æƒé‡
        self.patch_embed.apply(segm_init_weights)
        self.yolo_head.apply(segm_init_weights)
        self.yolo_head1.apply(segm_init_weights)
        self.yolo_head2.apply(segm_init_weights)
        # mamba init
        self.apply(
            partial(
                _init_weights,
                n_layer=depth,
                **(initializer_cfg if initializer_cfg is not None else {}),
            )
        )

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return {
            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
            for i, layer in enumerate(self.layers)
        }
    
    @torch.jit.ignore
    def no_weight_decay(self):
        return {"pos_embed", "cls_token", "dist_token", "cls_token_head", "cls_token_tail"}

    @torch.jit.ignore()
    def load_pretrained(self, checkpoint_path, prefix=""):
        _load_weights(self, checkpoint_path, prefix)
    
    
    
    def forward_features(self, x, pos_embed, condition=None):
        """ç‰¹å¾æå– - è¿”å›å¤šä¸ªä¸­é—´å±‚ç‰¹å¾å›¾
        
        Args:
            x: è¾“å…¥æ•°æ® (B, C, H, W, D)
            batch_dataset_ids: æ•°æ®é›†æ ‡è¯†ç¬¦ (B,)
            batch_params: æ•°æ®é›†å‚æ•°åˆ—è¡¨
        """
        # Patch embedding
        x = self.patch_embed(x)  # (B, N, C)
        x = x + pos_embed
        # x = self.pos_drop(x)

        B, N, C = x.shape

        features = []
        # feature_count = 0
        for i, layer in enumerate(self.layers):
            x = layer(x, cond_vector=condition)
            
            if i in self.feature_indices:
                features.append(x)
                # features.append(self.norm_layers[feature_count](x))
                # feature_count += 1
        
        return features  # è¿”å›å¤šä¸ªç‰¹å¾å›¾
    
    def tokens_to_3d_feature(self, tokens):
        """å°†tokensè½¬æ¢å›3Dç‰¹å¾å›¾"""
        B, N, C = tokens.shape
        H, W, D = self.patch_embed.grid_size
        x = tokens.transpose(1, 2).view(B, C, H, W, D)  # (B, C, H, W, D)
        
        return x
    
    def forward(self, x, condition, batch_params):

        if batch_params is not None:
            batch_params_tensor, has_velocity_mask = prepare_batch_params_tensor(batch_params, x.device)
            physical_coords = generate_physical_coords_fully_vectorized(
                batch_params_tensor, tuple(self.grid_tensor.tolist()), has_velocity_mask, x.device
            )
            # ç”Ÿæˆç‰©ç†ä½ç½®ç¼–ç 
            physical_pos_embed = get_3d_physical_pos_embed_dynamic(self.physical_pos_embed_dim, physical_coords)
            
            # ç”Ÿæˆæ¨¡æ€ç±»å‹åµŒå…¥
            modality_ids = has_velocity_mask.long()
            modality_embed = self.modality_embedding(modality_ids).unsqueeze(1)
            modality_embed = modality_embed.repeat(1, physical_pos_embed.shape[1], 1)
            
            # åˆå¹¶ç‰©ç†ä½ç½®ç¼–ç å’Œæ¨¡æ€åµŒå…¥
            pos_embed = torch.cat([physical_pos_embed, modality_embed], dim=-1)
        else:
            print("è­¦å‘Š: æ²¡æœ‰æä¾›ç‰©ç†å‚æ•°ï¼Œå›é€€åˆ°æ ‡å‡†çš„3Dæ­£å¼¦ä½ç½®ç¼–ç ã€‚")
            pos_embed_base = get_3d_sincos_pos_embed(
                self.embed_dim, tuple(self.grid_tensor.tolist()), cls_token=False
            )
            pos_embed = torch.from_numpy(pos_embed_base).float().unsqueeze(0).repeat(x.shape[0], 1, 1).to(x.device)



        multi_features = self.forward_features(x, pos_embed, condition=condition)
        
        # å°†å¤šä¸ªtokenç‰¹å¾è½¬æ¢ä¸º3Dç‰¹å¾å›¾
        grid_features = [self.tokens_to_3d_feature(features) for features in multi_features]
        
        # ä½¿ç”¨PANetè¿›è¡Œç‰¹å¾èåˆ
        feature0, feature1, feature2 = self.feature_fusion(grid_features)
        # print(feature0.shape)
        # print(feature1.shape)
        # print(feature2.shape)
        # YOLOæ£€æµ‹å¤´
        out0 = self.yolo_head(feature0)
        out1 = self.yolo_head1(feature1)
        out2 = self.yolo_head2(feature2)
        
        return out1, out2, out0  # ä¸radtr_yolo.pyä¿æŒä¸€è‡´çš„è¿”å›é¡ºåº

# ========= æ¨¡å‹æ³¨å†Œ =========
@register_model
def raddet_tiny(pretrained=False, **kwargs):
    model = RadDet(embed_dim=192, depth=12,  **kwargs)
    return model

# @register_model
# def raddet_tiny_bimamba_none(pretrained=False, **kwargs):
#     model = RadDet(embed_dim=192, depth=12, if_bimamba=False, bimamba_type="none", use_pgm=False, **kwargs)
#     return model

# @register_model
# def raddet_tiny_bimamba_none(pretrained=False, **kwargs):
#     model = RadDet(embed_dim=192, depth=12, if_bimamba=False, bimamba_type="none", **kwargs)
#     return model

# @register_model
# def raddet_tiny_bimamba_v4(pretrained=False, **kwargs):
#     model = RadDet(embed_dim=192, depth=12, if_bimamba=False, bimamba_type="v4", **kwargs)
#     return model

@register_model
def raddet_tiny_NOphysical(pretrained=False, **kwargs):
    model = RadDet(embed_dim=192, depth=12, **kwargs)
    return model

@register_model
def raddet_base(pretrained=False, **kwargs):
    model = RadDet(embed_dim=384, depth=16, **kwargs)
    return model

@register_model
def raddet_large(pretrained=False, **kwargs):
    model = RadDet(embed_dim=768, depth=24, **kwargs)
    return model


if __name__ == '__main__':
    # ===================================================================
    #                           æµ‹è¯•è®¾ç½®
    # ===================================================================
    print("æµ‹è¯• RadDet Tiny (Bi-Mamba None) ç‰ˆæœ¬...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨¡æ‹Ÿé…ç½®
    config_data = {}
    config_model = {"input_shape": [256, 256, 64]}
    anchor_boxes = [[16, 16, 32], [32, 32, 64], [64, 64, 128]]
    
    # [ä¿®æ”¹] å®šä¹‰æ‰¹å¤§å°å’Œæ¡ä»¶ç»´åº¦
    BATCH_SIZE = 2
    CONDITION_DIM = 12 # ä¸æ¨¡å‹å®šä¹‰çš„é»˜è®¤å€¼ condition_dim=12 åŒ¹é…
    
    print("\n" + "="*80)
    print(f"ğŸ“‹ å¼€å§‹æµ‹è¯• raddet_tiny_bimamba_none (Batch Size={BATCH_SIZE}, å¸¦FiLMæ¡ä»¶æ³¨å…¥)")
    print("="*80)
    
    # ===================================================================
    #                         åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    # ===================================================================
    print("ğŸ”Œ 1. å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥...")
    
    # a) ä¸»è¾“å…¥å¼ é‡ (RAD data)
    x_input = torch.randn(BATCH_SIZE, 1, 256, 256, 64).to(device)
    print(f"   - è¾“å…¥å¼ é‡ (x) shape: {x_input.shape}")
    
    # b) [æ–°å¢] FiLM æ¡ä»¶å‘é‡
    #    è¿™ä¸ªå‘é‡æ¨¡æ‹Ÿäº†ä»å…ƒæ•°æ®ç¼–ç å™¨è¾“å‡ºçš„ç»“æœ
    mock_condition_vector = torch.rand(BATCH_SIZE, CONDITION_DIM).to(device)
    print(f"   - FiLMæ¡ä»¶å‘é‡ (condition) shape: {mock_condition_vector.shape}")
    
    # c) PIPE ç‰©ç†å‚æ•°
    #    æ¨¡æ‹Ÿä¸€ä¸ªåŒ…å«ä¸åŒç±»å‹æ•°æ®çš„æ‰¹æ¬¡ (ä¸€ä¸ªæœ‰é€Ÿåº¦ï¼Œä¸€ä¸ªæ²¡æœ‰)
    mock_batch_params = [
        {'range_resolution': torch.tensor(0.1953), 'angular_resolution': torch.tensor(0.418), 'velocity_resolution': torch.tensor(0.4196), 'has_velocity': torch.tensor(True)},
        {'range_resolution': torch.tensor(0.115), 'angular_resolution': torch.tensor(0.469), 'velocity_resolution': torch.tensor(0.033), 'has_velocity': torch.tensor(False)} # æ¨¡æ‹ŸCRUWæ•°æ®
    ]
    # æ‰‹åŠ¨å°†å­—å…¸ä¸­çš„å¼ é‡å †å èµ·æ¥ï¼Œä»¥æ¨¡æ‹ŸDataLoaderçš„collate_fnè¡Œä¸º
    collated_batch_params = {k: torch.stack([d[k] for d in mock_batch_params]) for k in mock_batch_params[0]}
    print(f"   - ç‰©ç†å‚æ•° (batch_params) æ•°é‡: {len(mock_batch_params)}")
    
    # ===================================================================
    #                         æ¨¡å‹åˆ›å»ºä¸è¯„ä¼°
    # ===================================================================
    try:
        # 1. åˆ›å»ºæ¨¡å‹
        print("\nğŸ› ï¸ 2. åˆ›å»ºæ¨¡å‹å®ä¾‹...")
        model = raddet_tiny(
            config_data=config_data,
            config_model=config_model,
            anchor_boxes=anchor_boxes,
            condition_dim=CONDITION_DIM # æ˜¾å¼ä¼ é€’ä»¥ç¡®ä¿åŒ¹é…
        ).to(device).eval()
        
        print(f"âœ… å®é™…é…ç½®: embed_dim={model.embed_dim}, depth={model.depth}, condition_dim={CONDITION_DIM}")
        
        # 2. æ¨¡å‹å¤§å°è¯„ä¼°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {total_params/1e6:.2f}M")
        
        # 3. FLOPs åˆ†æ
        print("\nğŸ”¬ 3. å¼€å§‹è¿›è¡Œè¯¦ç»†çš„ FLOPs åˆ†æ (ä½¿ç”¨å•ä¸ªæ ·æœ¬)...")
        # ä¸ºFLOPsåˆ†æå‡†å¤‡å•ä¸€æ ·æœ¬è¾“å…¥
        single_input_for_flops = x_input[0:1]
        single_condition_for_flops = mock_condition_vector[0:1]
        single_params_for_flops = {k: v[0:1] for k, v in collated_batch_params.items()}

        try:
            # [ä¿®æ”¹] åœ¨FlopCountAnalysisä¸­ä¼ å…¥æ‰€æœ‰å‚æ•°
            flop_counter = FlopCountAnalysis(model, (single_input_for_flops, single_condition_for_flops, single_params_for_flops))
            total_flops = flop_counter.total()
            print(f"ğŸ“ˆ æ€» GFLOPs: {total_flops / 1e9:.4f} G")
            
            # (å¯é€‰) æ‰“å°è¯¦ç»†çš„æ¨¡å—åˆ†ææŠ¥å‘Š...
            
        except Exception as flops_error:
            print(f"âš ï¸ FLOPs è®¡ç®—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {flops_error}")
            import traceback
            traceback.print_exc()

        # 4. æ¨ç†å’Œè®¡æ—¶
        print("\nâ±ï¸ 4. å¼€å§‹è¿›è¡Œæ¨ç†æ—¶é—´æµ‹è¯•...")
        
        with torch.no_grad():
            if device.type == 'cuda':
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)
                # é¢„çƒ­
                for _ in range(5):
                    _ = model(x_input, condition=mock_condition_vector, batch_params=collated_batch_params)
                
                start_time.record()
                # [ä¿®æ”¹] ä¼ å…¥æ‰€æœ‰å‚æ•°è¿›è¡Œæ¨ç†
                outputs = model(x_input, condition=mock_condition_vector, batch_params=collated_batch_params)
                end_time.record()
                torch.cuda.synchronize()
                inference_time = start_time.elapsed_time(end_time)
            else:
                import time
                start = time.time()
                outputs = model(x_input, condition=mock_condition_vector, batch_params=collated_batch_params)
                end = time.time()
                inference_time = (end - start) * 1000

        print(f"ğŸ“¤ è¾“å‡º1å½¢çŠ¶: {outputs[0].shape} (è®¾å¤‡: {outputs[0].device})")
        print(f"ğŸ“¤ è¾“å‡º2å½¢çŠ¶: {outputs[1].shape} (è®¾å¤‡: {outputs[1].device})")
        print(f"ğŸ“¤ è¾“å‡º3å½¢çŠ¶: {outputs[2].shape} (è®¾å¤‡: {outputs[2].device})")
        print(f"â±ï¸ æ¨ç†æ—¶é—´ ({BATCH_SIZE}ä¸ªæ ·æœ¬): {inference_time:.2f}ms")
        print(f"\nâœ… raddet_tiny_bimamba_none ç‰ˆæœ¬æµ‹è¯•æˆåŠŸï¼")
        
        del model, outputs, x_input
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ raddet_tiny_bimamba_none ç‰ˆæœ¬æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "="*80)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*80)