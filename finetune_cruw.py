import torch
import torch.nn as nn
from functools import partial
from torch import Tensor
from typing import Optional, Tuple
import math
import numpy as np
import torch.nn.functional as F
from timm.models.vision_transformer import _load_weights
from fvcore.nn import FlopCountAnalysis, parameter_count_table
from timm.models.vision_transformer import _cfg
from timm.models.registry import register_model
from timm.models.layers import trunc_normal_, lecun_normal_, DropPath
from einops import rearrange, repeat
import inspect

from mamba_ssm.modules.mamba_simple import Mamba 
try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None

# ================= FiLM æ¨¡å—å®šä¹‰ =================
class FiLMLayer(nn.Module):
    """
    ç‰¹å¾çº§çº¿æ€§è°ƒåˆ¶å±‚ (FiLM Layer)ã€‚
    è¯¥å±‚æ ¹æ®ä¸€ä¸ªå¤–éƒ¨çš„æ¡ä»¶å‘é‡ï¼Œç”Ÿæˆç¼©æ”¾å› å­gammaå’Œåç§»å› å­betaï¼Œ
    å¹¶å°†å…¶åº”ç”¨äºè¾“å…¥çš„ç‰¹å¾å›¾ã€‚
    """
    def __init__(self, condition_dim, feature_dim):
        """
        åˆå§‹åŒ–FiLMå±‚ã€‚
        Args:
            condition_dim (int): æ¡ä»¶å‘é‡çš„ç»´åº¦ã€‚
            feature_dim (int): è¦è°ƒåˆ¶çš„ç‰¹å¾çš„ç»´åº¦ã€‚
        """
        super().__init__()
        # ä¸€ä¸ªç®€å•çš„å‰é¦ˆç½‘ç»œï¼Œç”¨äºä»æ¡ä»¶å‘é‡ç”Ÿæˆgammaå’Œbeta
        self.generator = nn.Sequential(
            nn.Linear(condition_dim, condition_dim * 2),
            nn.GELU(),
            nn.Linear(condition_dim * 2, feature_dim * 2) # è¾“å‡ºç»´åº¦æ˜¯ç‰¹å¾ç»´åº¦çš„ä¸¤å€ (gamma + beta)
        )

    def forward(self, x, cond_vector):
        """
        å‰å‘ä¼ æ’­ã€‚
        Args:
            x (Tensor): è¾“å…¥ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [B, N, C] (Batch, Num_Tokens, Channels)ã€‚
            cond_vector (Tensor): æ¡ä»¶å‘é‡ï¼Œå½¢çŠ¶ä¸º [B, D] (Batch, Condition_Dim)ã€‚
        
        Returns:
            Tensor: ç»è¿‡è°ƒåˆ¶çš„ç‰¹å¾ï¼Œå½¢çŠ¶ä¸xç›¸åŒã€‚
        """
        # 1. ç”Ÿæˆ gamma å’Œ beta
        # generatorçš„è¾“å‡ºå½¢çŠ¶ä¸º [B, C*2]
        gb = self.generator(cond_vector)
        
        # å°†gbåˆ‡åˆ†ä¸ºgammaå’Œbeta
        gamma, beta = torch.chunk(gb, 2, dim=1) # æ¯ä¸ªå½¢çŠ¶ä¸º [B, C]
        
        # 2. è°ƒæ•´å½¢çŠ¶ä»¥è¿›è¡Œå¹¿æ’­
        # [B, C] -> [B, 1, C]ï¼Œä»¥ä¾¿ä¸ [B, N, C] çš„xè¿›è¡Œå…ƒç´ çº§æ“ä½œ
        gamma = gamma.unsqueeze(1)
        beta = beta.unsqueeze(1)

        # 3. åº”ç”¨FiLMå˜æ¢: x' = Î³ * x + Î²
        # ä¸ºäº†è®­ç»ƒç¨³å®šæ€§ï¼Œé€šå¸¸åˆå§‹åŒ–æ—¶è®©gammaæ¥è¿‘1ï¼Œbetaæ¥è¿‘0ã€‚
        # è¿™é‡Œé€šè¿‡åŠ 1æ¥å®ç°ï¼Œä½¿å¾—åœ¨ç½‘ç»œåˆå§‹åŒ–æ—¶ï¼ŒFiLMå±‚è¿‘ä¼¼äºä¸€ä¸ªæ’ç­‰å˜æ¢ã€‚
        return (gamma + 1) * x + beta

# ================= å…ƒæ•°æ®ç¼–ç å™¨ =================
class MetadataEncoder(nn.Module):
    """
    å°†é›·è¾¾å…ƒæ•°æ®ç¼–ç ä¸ºFiLMæ‰€éœ€çš„æ¡ä»¶å‘é‡ã€‚
    """
    def __init__(self, num_datasets, numerical_dim, output_dim):
        """
        åˆå§‹åŒ–å…ƒæ•°æ®ç¼–ç å™¨ã€‚
        Args:
            num_datasets (int): æ•°æ®é›†ç±»å‹çš„æ•°é‡ (ä¾‹å¦‚: RADDet, CARRADAç­‰)ã€‚
            numerical_dim (int): æ•°å€¼å‹å…ƒæ•°æ®çš„ç»´åº¦ (ä¾‹å¦‚: [range_res, angle_res, vel_res])ã€‚
            output_dim (int): è¾“å‡ºæ¡ä»¶å‘é‡çš„ç›®æ ‡ç»´åº¦ã€‚
        """
        super().__init__()
        # 1. ç±»åˆ«å‹æ•°æ®ç¼–ç å™¨
        self.dataset_embedding_dim = 32 # ä¸ºæ•°æ®é›†IDåˆ†é…çš„åµŒå…¥ç»´åº¦
        self.dataset_embedder = nn.Embedding(num_datasets, self.dataset_embedding_dim)

        # 2. æ•°å€¼å‹æ•°æ®ç¼–ç å™¨ (ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚)
        self.numerical_proj_dim = 64
        self.numerical_projector = nn.Linear(numerical_dim, self.numerical_proj_dim)

        # 3. èåˆå±‚
        # å°†ç¼–ç åçš„ç±»åˆ«å‹å’Œæ•°å€¼å‹ç‰¹å¾æ‹¼æ¥åï¼Œé€šè¿‡ä¸€ä¸ªMLPè¿›è¡Œèåˆ
        total_input_dim = self.dataset_embedding_dim + self.numerical_proj_dim
        self.fusion_mlp = nn.Sequential(
            nn.Linear(total_input_dim, output_dim * 2),
            nn.GELU(),
            nn.Linear(output_dim * 2, output_dim)
        )

    def forward(self, dataset_ids, numerical_params):
        """
        å‰å‘ä¼ æ’­ã€‚
        Args:
            dataset_ids (Tensor): æ•°æ®é›†IDï¼Œå½¢çŠ¶ [B]ã€‚
            numerical_params (Tensor): æ•°å€¼å‹å‚æ•°ï¼Œå½¢çŠ¶ [B, numerical_dim]ã€‚

        Returns:
            Tensor: è¾“å‡ºçš„æ¡ä»¶å‘é‡ï¼Œå½¢çŠ¶ [B, output_dim]ã€‚
        """
        # ç¼–ç ç±»åˆ«å‹æ•°æ®
        dataset_emb = self.dataset_embedder(dataset_ids) # [B, 32]

        # ç¼–ç æ•°å€¼å‹æ•°æ®
        numerical_proj = self.numerical_projector(numerical_params) # [B, 64]
        
        # æ‹¼æ¥
        combined_features = torch.cat([dataset_emb, numerical_proj], dim=1) # [B, 32+64]

        # èåˆç”Ÿæˆæœ€ç»ˆçš„æ¡ä»¶å‘é‡
        cond_vector = self.fusion_mlp(combined_features) # [B, output_dim]
        
        return cond_vector

def get_3d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: ç½‘æ ¼é«˜åº¦ã€å®½åº¦å’Œæ·±åº¦çš„æ•´æ•°å…ƒç»„
    è¿”å›:
    pos_embed: [grid_depth*grid_height*grid_width, embed_dim] æˆ– [1+grid_depth*grid_height*grid_width, embed_dim] (å¸¦cls_token)
    """
    grid_d = np.arange(grid_size[0], dtype=np.float32)
    grid_h = np.arange(grid_size[1], dtype=np.float32)
    grid_w = np.arange(grid_size[2], dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h, grid_d)  # æ³¨æ„è¿™é‡Œçš„w, h, dé¡ºåºå¾ˆé‡è¦
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([3, 1, grid_size[1], grid_size[2], grid_size[0]])
    pos_embed = get_3d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed

def get_3d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 6 == 0

    # ä½¿ç”¨1/3ç»´åº¦ç»™grid_hï¼Œ1/3ç»™grid_wï¼Œ1/3ç»™grid_d
    emb_d = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[0])  # (H*W*D, D/3)
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[1])  # (H*W*D, D/3)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[2])  # (H*W*D, D/3)

    emb = np.concatenate([emb_d, emb_h, emb_w], axis=1) # (H*W*D, D)
    return emb

def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: æ¯ä¸ªä½ç½®çš„è¾“å‡ºç»´åº¦
    pos: è¦ç¼–ç çš„ä½ç½®åˆ—è¡¨ï¼šå¤§å° (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), å¤–ç§¯

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb

def get_3d_physical_pos_embed_dynamic(embed_dim, physical_coords):
    """
    åŠ¨æ€ç”Ÿæˆç‰©ç†ä½ç½®ç¼–ç ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰
    physical_coords: [batch_size, num_tokens, 3] ç‰©ç†åæ ‡ (è·ç¦», è§’åº¦, é€Ÿåº¦/æ—¶é—´)
    è¿”å›: [batch_size, num_tokens, embed_dim]
    """
    batch_size, num_tokens, _ = physical_coords.shape
    device = physical_coords.device
    
    # ç¡®ä¿ç»´åº¦èƒ½å¤Ÿå‡åŒ€åˆ†é…ç»™ä¸‰ä¸ªåæ ‡ç»´åº¦
    dim_per_coord = embed_dim // 3
    remaining_dims = embed_dim % 3
    
    pos_embed = torch.zeros(batch_size, num_tokens, embed_dim, device=device)
    
    start_dim = 0
    for j in range(3):  # è·ç¦»ã€è§’åº¦ã€é€Ÿåº¦/æ—¶é—´
        # ä¸ºå½“å‰åæ ‡åˆ†é…ç»´åº¦ï¼ˆç¬¬ä¸€ä¸ªåæ ‡è·å¾—é¢å¤–ç»´åº¦ï¼‰
        current_dim = dim_per_coord + (1 if j < remaining_dims else 0)
        
        if current_dim == 0:
            continue
            
        end_dim = start_dim + current_dim
        
        # ç”Ÿæˆé¢‘ç‡å‚æ•°ï¼ˆç¡®ä¿æ˜¯å¶æ•°ç»´åº¦ç”¨äºsin/coså¯¹ï¼‰
        if current_dim % 2 == 1:
            # å¦‚æœæ˜¯å¥‡æ•°ç»´åº¦ï¼Œä½¿ç”¨current_dim-1æ¥ç”Ÿæˆé¢‘ç‡
            freq_dim = current_dim - 1
            use_extra_dim = True
        else:
            freq_dim = current_dim
            use_extra_dim = False
            
        if freq_dim > 0:
            omega = torch.arange(freq_dim // 2, dtype=torch.float32, device=device)
            omega /= freq_dim / 2.
            omega = 1. / 10000**omega  # (freq_dim/2,)
            
            coord_data = physical_coords[:, :, j]  # [batch_size, num_tokens]
            
            # æ‰¹é‡è®¡ç®— (å®Œå…¨å‘é‡åŒ–)
            out = torch.einsum('bn,d->bnd', coord_data, omega)  # (batch_size, num_tokens, freq_dim/2)
            emb_sin = torch.sin(out)
            emb_cos = torch.cos(out)
            combined_emb = torch.cat([emb_sin, emb_cos], dim=-1)  # (batch_size, num_tokens, freq_dim)
        else:
            combined_emb = torch.zeros(batch_size, num_tokens, 0, device=device)
        
        # å¦‚æœéœ€è¦é¢å¤–ç»´åº¦ï¼Œæ·»åŠ ä¸€ä¸ªç®€å•çš„ç¼–ç 
        if use_extra_dim:
            extra_emb = physical_coords[:, :, j:j+1] * 0.01  # ç®€å•çš„çº¿æ€§ç¼–ç 
            combined_emb = torch.cat([combined_emb, extra_emb], dim=-1)
        
        pos_embed[:, :, start_dim:end_dim] = combined_emb
        start_dim = end_dim
    
    return pos_embed  # [batch_size, num_tokens, embed_dim]

def generate_physical_coords_fully_vectorized(batch_params_tensor, grid_size, has_velocity_mask, device):
    """
    å®Œå…¨å‘é‡åŒ–çš„ç‰©ç†åæ ‡ç”Ÿæˆï¼ˆæ— ä»»ä½•Pythonå¾ªç¯ï¼‰
    batch_params_tensor: [B, 4] çš„å¼ é‡, [range_res, angle_res, vel_res/time_res, is_velocity_flag]
    grid_size: (H, W, D)
    has_velocity_mask: [B] å¸ƒå°”å¼ é‡ï¼Œæ ‡è¯†æ¯ä¸ªæ ·æœ¬æ˜¯å¦æœ‰é€Ÿåº¦ç»´åº¦
    device: è®¡ç®—è®¾å¤‡
    è¿”å›: [batch_size, num_tokens, 3]
    """
    B = batch_params_tensor.shape[0]
    H, W, D = grid_size
    
    # 1. åˆ›å»ºç´¢å¼•ç½‘æ ¼ (h, w, d) - æ‰©å±•åˆ° [B, H, W, D]
    h_indices = torch.arange(H, device=device).view(1, H, 1, 1).expand(B, H, W, D)  # [B, H, W, D]
    w_indices = torch.arange(W, device=device).view(1, 1, W, 1).expand(B, H, W, D)  # [B, H, W, D]
    d_indices = torch.arange(D, device=device).view(1, 1, 1, D).expand(B, H, W, D)  # [B, H, W, D]
    
    # 2. å‡†å¤‡ç‰©ç†åˆ†è¾¨ç‡å¼ é‡ä»¥è¿›è¡Œå¹¿æ’­
    range_res = batch_params_tensor[:, 0].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    angle_res = batch_params_tensor[:, 1].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    vel_or_time_res = batch_params_tensor[:, 2].view(B, 1, 1, 1)  # [B, 1, 1, 1]
    
    # 3. ä½¿ç”¨å¹¿æ’­æœºåˆ¶ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åæ ‡ - å®Œå…¨å‘é‡åŒ–
    distances = h_indices * range_res  # [B, H, W, D]
    angles = (w_indices - W / 2) * angle_res  # [B, H, W, D]
    
    # å¤„ç†é€Ÿåº¦/æ—¶é—´ç»´åº¦çš„æ¡ä»¶å¹¿æ’­
    velocities = torch.where(
        has_velocity_mask.view(B, 1, 1, 1),
        (d_indices - D / 2) * vel_or_time_res,  # é€Ÿåº¦æ¨¡å¼
        d_indices * vel_or_time_res  # æ—¶é—´æ¨¡å¼ (CRUW)
    )  # [B, H, W, D]
    
    # 4. å †å å¹¶å±•å¹³ - å®Œå…¨å‘é‡åŒ–
    coords = torch.stack([distances, angles, velocities], dim=-1).view(B, -1, 3)
    
    # 5. æ‰¹å¤„ç†å½’ä¸€åŒ– (å…³é”®) - å®Œå…¨å‘é‡åŒ–
    max_vals = coords.max(dim=1, keepdim=True)[0]  # [B, 1, 3]
    min_vals = coords.min(dim=1, keepdim=True)[0]  # [B, 1, 3]
    range_vals = max_vals - min_vals
    range_vals = torch.where(range_vals > 0, range_vals, torch.ones_like(range_vals))
    
    coords_normalized = (coords - min_vals) / range_vals
    
    return coords_normalized

def prepare_batch_params_tensor(batch_params, device):
    """
    [ä¿®æ”¹åçš„ç‰ˆæœ¬]
    å°†å¼ é‡å­—å…¸æ ¼å¼çš„å‚æ•°ç›´æ¥è½¬æ¢ä¸ºæ‰€éœ€å¼ é‡ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰ã€‚
    batch_params: æ¯ä¸ªé”®éƒ½å¯¹åº”ä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º [B]ã€‚
    è¿”å›: [B, 4] å¼ é‡å’Œ [B] å¸ƒå°”æ©ç ã€‚
    """
    # ä»å­—å…¸ä¸­ç›´æ¥è·å–æ•´æ‰¹çš„å¼ é‡
    range_res = batch_params['range_resolution'].to(device)
    angle_res = batch_params['angular_resolution'].to(device)
    
    B = range_res.shape[0]
    default_vel_res = torch.full((B,), 0.05, device=device)
    vel_or_time_res = batch_params.get('velocity_resolution', batch_params.get('time_res', default_vel_res)).to(device)
    
    default_has_vel = torch.zeros(B, dtype=torch.bool, device=device)
    has_velocity = batch_params.get('has_velocity', default_has_vel).to(device)

    # åˆ›å»ºé€Ÿåº¦æ©ç å’Œé€Ÿåº¦æ ‡è®°
    has_velocity_mask = has_velocity.bool() 
    is_velocity_flag = has_velocity_mask.float()

    # ä½¿ç”¨ torch.stack å°†æ‰€æœ‰å‚æ•°å¼ é‡å †å èµ·æ¥
    batch_params_tensor = torch.stack([
        range_res.float(),
        angle_res.float(),
        vel_or_time_res.float(),
        is_velocity_flag
    ], dim=1)

    return batch_params_tensor, has_velocity_mask

class Double3DConvBlock(nn.Module):
    """ (3D conv => BN => ReLU) * 2 """
    def __init__(self, in_ch, out_ch, k_size=3, pad=1, dil=1):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv3d(in_ch, out_ch, kernel_size=k_size, padding=pad, dilation=dil, bias=False),
            nn.BatchNorm3d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_ch, out_ch, kernel_size=k_size, padding=pad, dilation=dil, bias=False),
            nn.BatchNorm3d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.block(x)
    
class PatchEmbed3D(nn.Module):
    """RAD tensor to 3D(RAD) Patch Embedding"""
    def __init__(self, input_size=(256, 256, 64), patch_size=(16, 16, 16), in_chans=1, embed_dim=192):
        super().__init__()
        self.input_size = input_size
        self.patch_size = patch_size
        
        # åŠ¨æ€è®¡ç®—ç½‘æ ¼å¤§å°
        self.grid_size = (
            input_size[0] // patch_size[0],  # H
            input_size[1] // patch_size[1],  # W
            input_size[2] // patch_size[2]   # D
        )
        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]

        self.proj = nn.Conv3d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        # è¾“å…¥xå½¢çŠ¶: [B, C, H, W, D]
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class SwiGLU(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.,
                 norm_layer=nn.LayerNorm, subln=False):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.w1 = nn.Linear(in_features, hidden_features)
        self.w2 = nn.Linear(in_features, hidden_features)

        self.act = act_layer()
        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()
        self.w3 = nn.Linear(hidden_features, out_features)

        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x1 = self.w1(x)
        x2 = self.w2(x)
        hidden = self.act(x1) * x2
        x = self.ffn_ln(hidden)
        x = self.w3(x)
        x = self.drop(x)
        return x

class ARBlock(nn.Module):
    def __init__(
        self, dim, mixer_cls, norm_cls=nn.LayerNorm, drop_path=0.,
        fused_add_norm=False, residual_in_fp32=False,
        use_film=False, condition_dim=None
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.mlp = SwiGLU(dim, dim*4*2//3, subln=False)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

        self.use_film = use_film
        if self.use_film:
            assert condition_dim is not None, "condition_dim must be provided if use_film is True"
            self.film_layer = FiLMLayer(condition_dim=condition_dim, feature_dim=dim)


    def forward(self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, cond_vector: Optional[Tensor] = None):
        normed_states = self.norm1(hidden_states)
        
        if self.use_film and cond_vector is not None:
            modulated_states = self.film_layer(normed_states, cond_vector)
        else:
            modulated_states = normed_states

        mixer_out = self.mixer(
            modulated_states, 
            inference_params=inference_params
        )

        hidden_states = hidden_states + self.drop_path(mixer_out)
        hidden_states = hidden_states + self.drop_path(self.mlp(self.norm2(hidden_states)))

        return hidden_states

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)

def create_block(
    d_model,
    ssm_cfg=None,
    norm_epsilon=1e-5,
    drop_path=0.,
    rms_norm=False,
    residual_in_fp32=False,
    fused_add_norm=False,
    layer_idx=None,
    device=None,
    dtype=None,
    if_bimamba=False,
    bimamba_type="none",
    if_devide_out=False,
    init_layer_scale=None,
    use_film=False,
    condition_dim=None,
):
    if if_bimamba:
        bimamba_type = "v1"
    if ssm_cfg is None:
        ssm_cfg = {}
    factory_kwargs = {"device": device, "dtype": dtype}
    mixer_cls = partial(Mamba, expand=1, layer_idx=layer_idx, bimamba_type=bimamba_type, if_divide_out=if_devide_out, init_layer_scale=init_layer_scale, **ssm_cfg, **factory_kwargs)
    norm_cls = partial(
        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
    )
    block = ARBlock(
        d_model,
        mixer_cls,
        norm_cls=norm_cls,
        drop_path=drop_path,
        fused_add_norm=fused_add_norm,
        residual_in_fp32=residual_in_fp32,
        use_film=use_film,
        condition_dim=condition_dim,
    )
    block.layer_idx = layer_idx
    return block

# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
def _init_weights(
    module,
    n_layer,
    initializer_range=0.02,  # Now only used for embedding layer.
    rescale_prenorm_residual=True,
    n_residuals_per_layer=1,  # Change to 2 if we have MLP
):
    if isinstance(module, nn.Linear):
        if module.bias is not None:
            if not getattr(module.bias, "_no_reinit", False):
                nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.
        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
        #
        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
                # We need to reinit p since this code could be called multiple times
                # Having just p *= scale would repeatedly scale it down
                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
                with torch.no_grad():
                    p /= math.sqrt(n_residuals_per_layer * n_layer)


def segm_init_weights(m):
    if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        lecun_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(m.bias)
        nn.init.ones_(m.weight)





class cruw(nn.Module):
    def __init__(
            self,
            n_classes=3,  # åˆ†å‰²ç±»åˆ«æ•°
            embed_dim=192,
            depth=12,
            ssm_cfg=None, 
            drop_path_rate=0,
            modality_embed_dim=64, # ### FIX 2: æ·»åŠ  modality_embed_dim å‚æ•°
            norm_epsilon: float = 1e-5, 
            rms_norm: bool = False, 
            initializer_cfg=None,
            fused_add_norm=False,
            residual_in_fp32=False,
            device=None,
            dtype=None,
            if_bimamba=False,
            bimamba_type="none",
            if_devide_out=False,
            init_layer_scale=None,
            use_film_metadata=True,
            condition_dim=12,
            **kwargs):
        
        factory_kwargs = {"device": device, "dtype": dtype}
        kwargs.update(factory_kwargs) 
        super().__init__()
        
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm

        self.embed_dim = embed_dim  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
        self.depth = depth          # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
        
        # æ¨¡å‹å‚æ•° - æ”¯æŒé…ç½®
        input_size = (256, 256, 64)
        patch_size = (8, 8, 8)  # ç»Ÿä¸€ä¸º(16, 16, 16)ï¼Œä¸é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸€è‡´
        # embed_dim å’Œ depth ç°åœ¨ä½œä¸ºå‚æ•°ä¼ å…¥
        
        print(f"å¾®è°ƒæ¨¡å‹åˆå§‹åŒ– - embed_dim: {embed_dim}, depth: {depth}")
        
        # Patch Embedding
        self.patch_embed = PatchEmbed3D(
            input_size=input_size,
            patch_size=patch_size, 
            in_chans=1,  # RADDetä½¿ç”¨1ä¸ªé€šé“
            embed_dim=embed_dim
        )
            
        self.grid_size = (
            input_size[0] // patch_size[0],
            input_size[1] // patch_size[1],
            input_size[2] // patch_size[2]
        )
        self.group_size = (
            min(4, self.grid_size[0]),
            min(4, self.grid_size[1]),
            min(4, self.grid_size[2])
        )
        print(f"ç½‘æ ¼å¤§å°: {self.grid_size}, åˆ†ç»„å¤§å°: {self.group_size}")
        self.register_buffer("grid_tensor", torch.tensor(self.grid_size, dtype=torch.long))
        self.register_buffer("group_tensor", torch.tensor(self.group_size, dtype=torch.long))

        # æ¨¡æ€ç±»å‹åµŒå…¥ (ä¿æŒåˆ†è¾¨ç‡ä¿¡æ¯)
        self.modality_embedding = nn.Embedding(2, modality_embed_dim)
        
        # è°ƒæ•´ç‰©ç†ä½ç½®ç¼–ç ç»´åº¦
        self.physical_pos_embed_dim = embed_dim - modality_embed_dim
        assert self.physical_pos_embed_dim > 0, "æ¨¡æ€åµŒå…¥ç»´åº¦ä¸èƒ½å¤§äºç­‰äºæ€»åµŒå…¥ç»´åº¦"
        print(f"ç‰©ç†ä½ç½®ç¼–ç ç»´åº¦: {self.physical_pos_embed_dim}, æ¨¡æ€åµŒå…¥ç»´åº¦: {modality_embed_dim}")

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        # import ipdb;ipdb.set_trace()
        inter_dpr = [0.0] + dpr    
        self.layers = nn.ModuleList(
            [
                create_block(
                    embed_dim,
                    ssm_cfg=ssm_cfg,
                    norm_epsilon=norm_epsilon,
                    rms_norm=rms_norm,
                    residual_in_fp32=residual_in_fp32,
                    fused_add_norm=fused_add_norm,
                    layer_idx=i,
                    if_bimamba=if_bimamba,
                    bimamba_type=bimamba_type,
                    drop_path=inter_dpr[i],
                    if_devide_out=if_devide_out,
                    init_layer_scale=init_layer_scale,
                    use_film=use_film_metadata,
                    condition_dim=condition_dim, 
                    **factory_kwargs,
                )
                for i in range(depth)
            ]
        )

        self.norm = nn.LayerNorm(self.embed_dim, eps=norm_epsilon)

        num_stages = 2 # e.g., patch_size=8 -> 3 stages

        # è‡ªåŠ¨è®¡ç®—æ¯å±‚è§£ç å™¨çš„é€šé“æ•°ï¼Œé€å±‚å‡åŠ
        decoder_dims = [embed_dim] + [embed_dim // (2**i) for i in range(1, num_stages + 1)]
        # ä¾‹: embed_dim=192, patch_size=8 -> num_stages=3 -> decoder_dims = [192, 96, 48, 24]
        
        print(f"è§£ç å™¨é˜¶æ®µæ•°: {num_stages}, è§£ç å™¨é€šé“ç»´åº¦: {decoder_dims}")

        self.decoder_blocks = nn.ModuleList()
        for i in range(num_stages):
            in_dim = decoder_dims[i]
            out_dim = decoder_dims[i+1]
            
            # 3Dä¸Šé‡‡æ · + 3DåŒå·ç§¯å—
            if i==0:
                up_layer = nn.ConvTranspose3d(in_dim, out_dim, kernel_size=(1,2,2), stride=(1,2,2))
            else:
                up_layer = nn.ConvTranspose3d(in_dim, out_dim, kernel_size=2, stride=2)    
            
            conv_block = Double3DConvBlock(out_dim, out_dim)
            
            self.decoder_blocks.append(nn.ModuleList([up_layer, conv_block]))
        final_decoder_dim = decoder_dims[-1]
        self.final_conv = nn.Conv3d(final_decoder_dim, n_classes, kernel_size=1)  

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, (nn.Conv3d, nn.Linear, nn.ConvTranspose3d)):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm3d)):
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
            if m.weight is not None:
                nn.init.constant_(m.weight, 1.0)

    def tokens_to_3d_feature(self, tokens):
        """å°†tokensè½¬æ¢å›3Dç‰¹å¾å›¾"""
        B, N, C = tokens.shape
        H, W, D = self.patch_embed.grid_size
        x = tokens.transpose(1, 2).view(B, C, H, W, D)  # (B, C, H, W, D)
        
        return x
    
    def forward_feature(self, x, pos_embed, condition=None):
        x = self.patch_embed(x)  # (B, N, C)
        x = x + pos_embed
        for layer in self.layers:
            x = layer(x, cond_vector=condition)
            
        x = self.norm(x)

        x = self.tokens_to_3d_feature(x)

        return x
        
    def forward(self, x, condition, batch_params):
        if batch_params is not None:
            batch_params_tensor, has_velocity_mask = prepare_batch_params_tensor(batch_params, x.device)
            physical_coords = generate_physical_coords_fully_vectorized(
                batch_params_tensor, tuple(self.grid_tensor.tolist()), has_velocity_mask, x.device
            )
            # ç”Ÿæˆç‰©ç†ä½ç½®ç¼–ç 
            physical_pos_embed = get_3d_physical_pos_embed_dynamic(self.physical_pos_embed_dim, physical_coords)
            
            # ç”Ÿæˆæ¨¡æ€ç±»å‹åµŒå…¥
            modality_ids = has_velocity_mask.long()
            modality_embed = self.modality_embedding(modality_ids).unsqueeze(1)
            modality_embed = modality_embed.repeat(1, physical_pos_embed.shape[1], 1)
            
            # åˆå¹¶ç‰©ç†ä½ç½®ç¼–ç å’Œæ¨¡æ€åµŒå…¥
            pos_embed = torch.cat([physical_pos_embed, modality_embed], dim=-1)
        else:
            print("è­¦å‘Š: æ²¡æœ‰æä¾›ç‰©ç†å‚æ•°ï¼Œå›é€€åˆ°æ ‡å‡†çš„3Dæ­£å¼¦ä½ç½®ç¼–ç ã€‚")
            pos_embed_base = get_3d_sincos_pos_embed(
                self.embed_dim, tuple(self.grid_tensor.tolist()), cls_token=False
            )
            pos_embed = torch.from_numpy(pos_embed_base).float().unsqueeze(0).repeat(x.shape[0], 1, 1).to(x.device)

        x = self.forward_feature(x, pos_embed, condition=condition)
        x = x.permute(0, 1, 4, 2, 3)  # (B, C, H, W, D)
        for up_layer, conv_block in self.decoder_blocks:
            x = up_layer(x)
            x = conv_block(x)

        output = self.final_conv(x)

        return output 
    
def cruw_tiny(**kwargs):
    model = cruw(
          embed_dim=192,
          depth=12,
        **kwargs,
    )
    return model


# --- Example Usage (Corrected) ---
if __name__ == "__main__":
    import inspect

    # ===================================================================
    #                         æµ‹è¯•è®¾ç½®
    # ===================================================================
    print("="*80)
    print("                     æµ‹è¯• cruw (cruw_tiny) ç‰ˆæœ¬")
    print("="*80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å®šä¹‰æ‰¹å¤§å°å’Œæ¨¡å‹å‚æ•°
    BATCH_SIZE = 2
    
    # --- FIX START: ä¿®æ­£åŠ¨æ€è·å–å‚æ•°çš„é€»è¾‘ ---
    # ä»æ¨¡å‹ __init__ çš„é»˜è®¤å‚æ•°ä¸­å®‰å…¨åœ°è·å–å€¼
    init_signature = inspect.signature(cruw.__init__)
    CONDITION_DIM = init_signature.parameters['condition_dim'].default
    N_CLASSES = init_signature.parameters['n_classes'].default
    # --- FIX END ---
    
    print(f"   - æ£€æµ‹åˆ°æ¨¡å‹å‚æ•°: N_CLASSES={N_CLASSES}, CONDITION_DIM={CONDITION_DIM}")
    
    # å®šä¹‰è¾“å…¥å°ºå¯¸
    INPUT_SIZE = (256, 256, 64) # H, W, D (ç¬¦åˆ __init__ ä¸­çš„å®šä¹‰)

    # ===================================================================
    #                         åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    # ===================================================================
    print("\nğŸ”Œ 1. å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥...")
    
    # a) ä¸»è¾“å…¥å¼ é‡ (RAD data)
    x_input = torch.randn(BATCH_SIZE, 1, *INPUT_SIZE).to(device)
    print(f"   - è¾“å…¥å¼ é‡ (x) shape: {x_input.shape}")
    
    # b) FiLM æ¡ä»¶å‘é‡
    mock_condition_vector = torch.rand(BATCH_SIZE, CONDITION_DIM).to(device)
    print(f"   - FiLMæ¡ä»¶å‘é‡ (condition) shape: {mock_condition_vector.shape}")
    
    # c) ç‰©ç†å‚æ•°
    mock_batch_params = [
        {'range_resolution': torch.tensor(0.1953), 'angular_resolution': torch.tensor(0.418), 'velocity_resolution': torch.tensor(0.4196), 'has_velocity': torch.tensor(True)},
        {'range_resolution': torch.tensor(0.115), 'angular_resolution': torch.tensor(0.469), 'time_res': torch.tensor(0.033), 'has_velocity': torch.tensor(False)}
    ]
    collated_batch_params = {
        key: torch.stack([d.get(key, torch.tensor(0.0 if key != 'has_velocity' else False)) for d in mock_batch_params])
        for key in mock_batch_params[0].keys()
    }
    print(f"   - ç‰©ç†å‚æ•° (batch_params) å·²å‡†å¤‡, æ‰¹æ¬¡å¤§å°: {len(mock_batch_params)}")
    
    # ===================================================================
    #                       æ¨¡å‹åˆ›å»ºä¸è¯„ä¼°
    # ===================================================================
    try:
        # 1. åˆ›å»ºæ¨¡å‹
        print("\nğŸ› ï¸ 2. åˆ›å»ºæ¨¡å‹å®ä¾‹ (cruw_tiny)...")
        # ä¸ºäº†è¿è¡Œæ­¤è„šæœ¬ï¼Œè¯·ç¡®ä¿åœ¨ä½¿ç”¨å‰å¯¼å…¥ inspect æ¨¡å—
        import inspect 
        model = cruw_tiny(
            n_classes=N_CLASSES,
            condition_dim=CONDITION_DIM,
        ).to(device).eval()
        
        # 2. æ¨¡å‹å¤§å°è¯„ä¼°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {total_params/1e6:.2f}M")
        
        # 3. FLOPs åˆ†æ (å¯é€‰)
        print("\nğŸ”¬ 3. å¼€å§‹è¿›è¡Œ FLOPs åˆ†æ (ä½¿ç”¨å•ä¸ªæ ·æœ¬)...")
        single_input = x_input[0:1]
        single_condition = mock_condition_vector[0:1]
        single_params = {k: v[0:1] for k, v in collated_batch_params.items()}

        try:
            flop_counter = FlopCountAnalysis(model, (single_input, single_condition, single_params))
            total_flops = flop_counter.total()
            print(f"ğŸ“ˆ æ€» GFLOPs: {total_flops / 1e9:.4f} G")
        except Exception as flops_error:
            print(f"âš ï¸ FLOPs è®¡ç®—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {flops_error}")

        # 4. æ¨ç†å’Œè®¡æ—¶
        print("\nâ±ï¸ 4. å¼€å§‹è¿›è¡Œæ¨ç†...")
        with torch.no_grad():
            for _ in range(5):
                _ = model(x_input, condition=mock_condition_vector, batch_params=collated_batch_params)
            
            if device.type == 'cuda':
                start_event = torch.cuda.Event(enable_timing=True)
                end_event = torch.cuda.Event(enable_timing=True)
                start_event.record()

            output_seg = model(x_input, condition=mock_condition_vector, batch_params=collated_batch_params)
            print(f"   - è¾“å‡ºå¼ é‡ (segmentation) shape: {output_seg.shape}")

            if device.type == 'cuda':
                end_event.record()
                torch.cuda.synchronize()
                inference_time = start_event.elapsed_time(end_event)
                print(f"   - GPU æ¨ç†æ—¶é—´ ({BATCH_SIZE}ä¸ªæ ·æœ¬): {inference_time:.2f}ms")

        # ===================================================================
        #                         è¾“å‡ºéªŒè¯
        # ===================================================================
        print("\nâœ… 5. æ¨ç†å®Œæˆï¼ŒéªŒè¯è¾“å‡º...")
        expected_shape = (BATCH_SIZE, N_CLASSES, 16, 128, 128)
        print(f"   - è¾“å‡ºå¼ é‡ shape: {output_seg.shape}")
        print(f"   - é¢„æœŸè¾“å‡º shape: {expected_shape}")

        assert output_seg.shape == expected_shape, "è¾“å‡ºå½¢çŠ¶ä¸é¢„æœŸä¸åŒ¹é…!"
        print("\nğŸ‘ è¾“å‡ºå½¢çŠ¶éªŒè¯æˆåŠŸ!")

        if device.type == 'cuda':
            print("\n--- GPU ä¿¡æ¯ ---")
            print(f"Device Name: {torch.cuda.get_device_name(0)}")
            print(f"Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB")
            print(f"Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "="*80)
    print("ğŸ‰ æµ‹è¯•æµç¨‹ç»“æŸï¼")
    print("="*80)