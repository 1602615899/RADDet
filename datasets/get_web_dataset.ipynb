{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     sink\u001b[38;5;241m.\u001b[39mwrite(data) \u001b[38;5;66;03m# 将数据写入.tar文件\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43msink\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 关闭.tar文件\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/writer.py:345\u001b[0m, in \u001b[0;36mTarWriter.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarstream\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mown_fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mown_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mown_fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data_name = 'CARRADA'\n",
    "data_name = 'RADDet'\n",
    "fold_idx = '01'\n",
    "\n",
    "if data_name == 'CARRADA':\n",
    "    data_dir = '/mnt/SrvDataDisk/Datasets_Radar/CARRADA'\n",
    "    sequences_all = glob.glob(os.path.join(data_dir, \"Carrada_RAD\", \"*/*/*.npy\"))\n",
    "elif data_name == 'RADDet':\n",
    "    data_dir = '/mnt/Disk/zx/data/RADDet_author'\n",
    "    sequences_train = glob.glob(os.path.join(data_dir, \"train\", \"RAD/*/*.npy\"))\n",
    "    sequences_test = glob.glob(os.path.join(data_dir, \"test\", \"RAD/*/*.npy\"))\n",
    "    sequences_all = sequences_train + sequences_test\n",
    "sequences_all.sort()\n",
    "\n",
    "index_start = 0\n",
    "index_end = 10\n",
    "# 0-1000, 1000-2001, 2001-3001, 3001-4001, 4001-5001, 5001-6001, 6001-7001, 7001-8001, 8001-9001, 9001-:\n",
    "sequence = sequences_all[index_start:index_end]\n",
    "\n",
    "sink = wds.TarWriter(f\"{data_name}_{fold_idx}.tar\") # 使用TarWriter，准备将数据写入.tar文件\n",
    "\n",
    "index = index_start\n",
    "for filename in tqdm(sequence):\n",
    "    sample = np.load(filename)\n",
    "    data = {\n",
    "        \"__key__\": f\"sample_{index:06d}\",  # 当前样本的index\n",
    "        \"input.npy\": sample,  # 雷达数据\n",
    "    }\n",
    "    sink.write(data) # 将数据写入.tar文件\n",
    "    index += 1\n",
    "sink.close() # 关闭.tar文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample_000053',)\n",
      "torch.Size([1, 256, 256, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/pipeline.py\", line 71, in iterator\n    for sample in self.iterator1():\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/filters.py\", line 305, in _map\n    for sample in data:\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/filters.py\", line 217, in _shuffle\n    for sample in data:\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/compat.py\", line 105, in check_empty\n    raise ValueError(\"No samples found in dataset; perhaps you have fewer shards than workers.\\n\" +\nValueError: No samples found in dataset; perhaps you have fewer shards than workers.\nTurn off using empty_check=False in the WebDataset constructor.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\n\u001b[1;32m     25\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     26\u001b[0m     dataset, \n\u001b[1;32m     27\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# shuffle=True,     # 这里不能用shuffle？\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1445\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1444\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/pipeline.py\", line 71, in iterator\n    for sample in self.iterator1():\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/filters.py\", line 305, in _map\n    for sample in data:\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/filters.py\", line 217, in _shuffle\n    for sample in data:\n  File \"/home/ljm/anaconda3/envs/rpt/lib/python3.10/site-packages/webdataset/compat.py\", line 105, in check_empty\n    raise ValueError(\"No samples found in dataset; perhaps you have fewer shards than workers.\\n\" +\nValueError: No samples found in dataset; perhaps you have fewer shards than workers.\nTurn off using empty_check=False in the WebDataset constructor.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from io import BytesIO\n",
    "\n",
    "def decode_npy(sample):\n",
    "    # sample 是一个包含相关信息的字典\n",
    "    byte_data = sample['input.npy']  # 获取字节数据\n",
    "    # 使用 BytesIO 将字节数据转换为文件对象\n",
    "    data = np.load(BytesIO(byte_data))  # 加载 .npy 文件\n",
    "\n",
    "    index = sample['__key__']  # 获取索引信息\n",
    "    return index, data\n",
    "\n",
    "dataset = (\n",
    "    # wds.WebDataset(\"CARRADA_{01..02}.tar\", shardshuffle=True)\n",
    "    wds.WebDataset(\"CARRADA_01.tar\", shardshuffle=True)\n",
    "    .shuffle(100)\n",
    "    .map(decode_npy)\n",
    "    .with_epoch(10)\n",
    "    .with_length(100)\n",
    "    # .decode()\n",
    "    # .to_tuple(\"input.npy\")\n",
    ")\n",
    "batch = next(iter(dataset))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=1, \n",
    "    # shuffle=True,     # 这里不能用shuffle？\n",
    "    num_workers=2  \n",
    ")\n",
    "for idx, batch in dataloader:\n",
    "    print(idx)\n",
    "    print(batch.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 500], [500, 1000], [1000, 1500], [1500, 2000], [2000, 2500], [2500, 3000], [3000, 3500], [3500, 4000], [4000, 4500], [4500, 5000], [5000, 5500], [5500, 6000], [6000, 6500], [6500, 7000], [7000, 7500], [7500, 8000], [8000, 8500], [8500, 9000], [9000, 9500], [9500, 10000], [10000, 10500], [10500, 11000], [11000, 11500], [11500, 12000], [12000, 12500], [12500, 12666]]\n"
     ]
    }
   ],
   "source": [
    "index_list = []\n",
    "gap = 500\n",
    "len_all = 12666\n",
    "for start in range(0, len_all, gap):\n",
    "    end = min(start + gap, len_all)  # 确保不会超出范围\n",
    "    index_list.append([start, end])\n",
    "\n",
    "print(index_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
